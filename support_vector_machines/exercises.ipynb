{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. What is the fundamental idea behind Support Vector Machines?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental idea behind Support Vector Machines is to fit the widest possible \"street\" between the class. In other words, the goal is to have the largest possible margin between the decision boundary that separates the two classes and the training instances. When performing soft margin classification, the SVM searches for a compromise between perfectly separating the two classes and having the widest possible street (i.e., a few instances may end up on the street). Another key idea is to use kernels when training on nonlinear datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. What is a support vector?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the SVM, a support vector is any instance located **on the \"street\"**, including its border. The decision boundary is entirely determined by the support vectors. Any instance that is not a support vector has no influence whatsoever; you could remove them, add more instances, or move them around, and as long as they stay off the street they won't affect the decisoin boundary. **Computing the predictions only involves the support vectors, not the whole trainig set.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Why is it important to scale the inputs when using SVMs?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs try to fit the largest possible \"street\" between the classes, so if the training set is not scaled, the SVM will tend to neglect small features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An SVM classifier can output the distance between the test instance and the decision boundary, and you can use this as a confidence score. However, this score cannot be directly converted into an estimation of the class probability. If you set `probability=True` when creating an SVM in Scikit-Learn, then after training it will calibrate the probabilities using Logistic Regression on the SVM's scores (trained by an additional five-fold cross-validation on the training data). This will add the `predict_proba()` and `predict_log_proba()` methods to the SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Should you use the primal or the dual form of the SVM problem to train a model on a training set with millions of instances and hundreds of features?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should you increase or decrease gamma? What about C?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. How should you set the QP parameters (H, f, A and b) to solve the soft margin linear SVM classifer problem using an off-the-shelf QP slover?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
